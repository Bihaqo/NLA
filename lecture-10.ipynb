{
 "metadata": {
  "name": "",
  "signature": "sha256:25edbd26e593d0f4693c591680a44ff377ac7453746496a4861f26b7aa5a772b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Lecture 10: Iterative methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Syllabus\n",
      "**Week 1:** Python intro  \n",
      "**Week 2:** Matrices, vectors, norms, ranks  \n",
      "**Week 3:** Linear systems, eigenvectors, eigenvalues  \n",
      "**Week 4:** Singular value decomposition + test + homework seminar  \n",
      "**Week 5:** Sparse & structured matrices  \n",
      "**Week 6:** Iterative methods, preconditioners, matrix functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Recap of the previous lecture\n",
      "- Shift-invariant structure: Toeplitz matrices\n",
      "- Circulant matrices and FFT\n",
      "- What is an inverse of a Toeplitz matrix?\n",
      "- Displacement rank (and Gohberg-Semencul formula)\n",
      "- Multilevel matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Today lecture\n",
      "Today we will talk about **iterative methods**. List of topics (we will possibly not cover all) includes\n",
      "\n",
      "- Concept of iterative methods\n",
      "- Richardson iteration, Chebyshev acceleration\n",
      "- Jacobi/Gauss-Seidel methods\n",
      "- Idea of Krylov methods\n",
      "- Conjugate gradient methods for symmetric positive definite matrices\n",
      "- Generalized minimal residual methods\n",
      "- The Zoo of iterative methods: BiCGStab, QMR, IDR, ...\n",
      "- The idea of preconditioners\n",
      "- Jacobi/Gauss-Seidel as preconditioner, successive overrelaxation\n",
      "- Incomplete ILU"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Iterative methods: basic idea\n",
      "\n",
      "The basic idea of an iterative method is simple. Suppose we have a sparse matrix (or structured) matrix $A$ and we can compute $Ax$ fast. A typical inversion algorithm is $\\mathcal{O}(N^3)$. So, maybe we can just multiply a matrix by a vector **many times** instead. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Richardson iteration\n",
      "The simplest idea is the **\"simple iteration method\"** or **Richardson iteration**.  \n",
      "\n",
      "\n",
      "  $$Ax = f,$$\n",
      "  $$\\tau  (Ax - f) = 0,$$\n",
      "   $$x + \\tau (Ax - f) = x,$$\n",
      "   $$x_{k+1} = x_k + \\tau (Ax_k - f).$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Convergence of the Richardson method\n",
      "Let $x_*$ be the solution; introduce an error $e_k = x_{k} - x_*$, then  \n",
      "$$\n",
      "     e_{k+1} = (I + \\tau A) e_k,\n",
      "$$\n",
      "therefore if $\\Vert I + \\tau A \\Vert < 1$ the iteration converges. For symmetric positive definite case it is always possible to select $\\tau$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optimal parameter choice\n",
      "The optimal choice for $\\tau$ for $A = A^* > 0$ is  \n",
      "$$\n",
      "  \\tau = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Different time steps\n",
      "Suppose we **change** $\\tau$ every step, i.e. \n",
      "$$\n",
      "   x_{k+1} = x_k + \\tau_k (A x_k - f).\n",
      "$$\n",
      "\n",
      "Then, $$e_{k+1} = (I - \\tau_k A) e_k = (I - \\tau_k A) (I - \\tau_{k-1} A)  e_{k-1} = \\ldots = p(A) e_0, $$\n",
      "where $p(A)$ is a **matrix polynomial** (simplest matrix function)  \n",
      "$$\n",
      "   p(A) = (I - \\tau_k A) \\ldots (I - \\tau_0 A),\n",
      "$$\n",
      "and $p(0) = 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chebyshev polynomials\n",
      "For $A = A^*$ the problem reduces to finding a polynomial $p(w)$ such that $p(0) = 1$ and it is a close to $0$ as possible on \n",
      "$[\\lambda_{\\min}, \\lambda_{\\max}].$  \n",
      "The answer on $[-1, 1]$ is given by **Chebyshev polynomials**, and on a general interval - by their scaled version."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot some Chebyshev polynomials"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "x = np.linspace(-1, 1, 128)\n",
      "p = np.polynomial.Chebyshev((0, 0, 0, 0, 0, 0, 0, 1), (-1, 1)) #These are Chebyshev series, a proto of \"chebfun system\" in MATLAB\n",
      "plt.plot(x, p(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import scipy as sp\n",
      "import scipy.sparse\n",
      "import scipy.sparse.linalg as spla\n",
      "import scipy\n",
      "from scipy.sparse import csc_matrix\n",
      "n = 20\n",
      "ex = np.ones(n);\n",
      "lp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
      "rhs = np.ones(n)\n",
      "ev1, vec = spla.eigs(lp1, k=2, which='LR')\n",
      "ev2, vec = spla.eigs(lp1, k=2, which='SR')\n",
      "lam_max = ev1[0]\n",
      "lam_min = ev2[0]\n",
      "\n",
      "tau_opt = 2.0/(lam_max + lam_min)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "plt.close(fig)\n",
      "\n",
      "niters = 100\n",
      "x = np.zeros(n)\n",
      "res_all = []\n",
      "for i in xrange(niters):\n",
      "    rr = lp1.dot(x) - rhs\n",
      "    x = x + tau_opt * rr\n",
      "    res_all.append(np.linalg.norm(rr))\n",
      "#Convergence of an ordinary Richardson (with optimal parameter)\n",
      "plt.plot(res_all)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chebyshev and beyond\n",
      "Chebyshev method is efficient when you know that your spectrum is on an interval.  \n",
      "But what happens if the spectrum has not that nice structure?  \n",
      "For example, if the spectrum is contained on **two intervals**, the solution can be  \n",
      "written in terms of **Zolotarev polynomials** and **elliptic functions**, and for three intervals  \n",
      "you have to go to the **hyperelliptic functions**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Krylov subspaces\n",
      "The Chebyshev method produces the approximation in the **Krylov subspace** of the form  \n",
      "$$\n",
      "   K(A, f) = \\mathrm{Span}(f, Af, A^2 f, \\ldots, )\n",
      "$$\n",
      "Then we can minimize the **residual**: \n",
      "\n",
      "$$\\Vert f - Ax \\Vert,$$\n",
      "over all $x \\in \\mathcal{K}_n(A, f)$, where the norms can be different (and even strange)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conjugate gradient method \n",
      "Conjugate gradient method (CG) is the best method for solving symmetric positive definite systems, if the only information about the system is the **matrix-by-vector product**. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Idea of the CG method\n",
      "Then, we can introduce a **A-norm** as\n",
      "$$\n",
      "   \\Vert x \\Vert_A = \\sqrt{(Ax, x)},\n",
      "$$\n",
      "and minimize the **A-norm** of the error. Let $x_*$ be the true solution of $Ax = f$, then we want to minimize the A-norm of \n",
      "$$\n",
      "    \\Vert x_k - x_* \\Vert_A, \n",
      "$$\n",
      "over the $n$-th Krylov subspace."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CG formulas\n",
      "$x_i = x_0 + y_i, \\quad y_i = \\arg \\min \\Vert x_0 + y - z \\Vert_A, \\quad y \\in K_n$.  \n",
      "\n",
      "Using Pythagoreus theorem,\n",
      "\n",
      "$(x_n, y) = (z, y)_A$ for all $y \\in K_n$, $\\rightarrow$ $r_n = Ax_n - f \\perp K_n$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CG-method: formulas\n",
      "$$\n",
      "   \\alpha_n = (r_{n-1}, r_{n-1})/(Ap_n, p_n),\n",
      "$$\n",
      "$$\n",
      "   x_n = x_{n-1} + \\alpha_n p_n, \n",
      "$$\n",
      "$$\n",
      "   r_n = r_{n-1} - \\alpha_n A p_n,\n",
      "$$\n",
      "$$\n",
      "   \\beta_n = (r_n, r_n)/(r_{n-1},r_{n-1}),\n",
      "$$\n",
      "$$\n",
      "    p_{n+1} = r_n + \\beta_n p_n\n",
      "$$\n",
      "\n",
      "\n",
      "The vectors $p_i$ constitute an A-orthogonal basis in $K_n$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CG-method: history\n",
      "1. In was proposed in by [Hestenes and Stiefel in 1952](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf).\n",
      "2. In exact arithmetics it should give **exact solution** at $n$ iterations\n",
      "3. In floating-point arithmetics it did not - people thought it is **unstable** compared to Gaussian elimination\n",
      "4. Only decades later it was realized that it is a wonderful **iterative method**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## CG-method: properties\n",
      "\n",
      "1. Convergence estimate: \n",
      "  $$\n",
      "     \\Vert e_i \\Vert \\leq 2 \\Big( \\frac{1 - \\sqrt{\\frac{m}{M}}}{1 + \\sqrt{\\frac{m}{M}}} \\Big)^i \\Vert e_0 \\Vert.\n",
      "  $$\n",
      "  \n",
      "2. Clustering of eigenvalues: if there are $k$ \"outliers\", then in $k$ iterations they are \"removed\" (we can illustrate it on the board)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## GMRES\n",
      "For non-symmetric matrices, there are two methods that are typically used: GMRES and BiCGStab.  \n",
      "\n",
      "GMRES idea is  very simple: we construct orthogonal basis in the Krylov subspace.  \n",
      "Given the basis $q_1, \\ldots, q_n$ we compute the residual $r_ n = Ax_n - f$ and orthogonalize it to the basis.\n",
      "\n",
      "A Python realization is available as ```scipy.sparse.linalg.gmres``` (although quite buggy)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Disadvantage of GMRES\n",
      "The main disadvantage of GMRES: we have to store all the vectors, so the memory costs grows with each step.  \n",
      "We can do **restarts** (i.e. get a new residual and a new Krylov subspace). BiCGSStab method avoids that using \n",
      "\"short recurrences\" like in the CG method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Idea of BiCGStab\n",
      "\n",
      "Use CG method for $A^{\\top} A x = A^{\\top}f$. The condition number is squared, thus a certain stablilization was proposed\n",
      "by Van der Vorst et al. Used a lot in practice for solving non-symmetric linear systems, it requires product by $A$ and $A^{\\top}$\n",
      "at one iteration step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can do a short demo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.sparse.linalg\n",
      "\n",
      "\n",
      "n = 50\n",
      "ex = np.ones(n);\n",
      "lp1 = -sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
      "rhs = np.ones(n)\n",
      "ee = sp.sparse.eye(n)\n",
      "\n",
      "#lp2 = sp.kron(lp1, ee) + sp.kron(ee, lp1)\n",
      "#rhs = np.ones(n * n)\n",
      "res_all = []\n",
      "res_all_bicg = []\n",
      "def my_print(r):\n",
      "    res_all.append(r)\n",
      "\n",
      "def my_print2(x): #For BiCGStab they have another callback, please rewrite\n",
      "    res_all_bicg.append(np.linalg.norm(lp1.dot(x) - rhs))\n",
      "    \n",
      "sol = scipy.sparse.linalg.gmres(lp1, rhs, callback=my_print)\n",
      "plt.semilogy(res_all, marker='x',label='GMRES')\n",
      "sol2 = scipy.sparse.linalg.bicgstab(lp1, rhs, callback=my_print2)\n",
      "plt.xlabel('Iteration number')\n",
      "plt.ylabel('Residual')\n",
      "plt.semilogy(res_all_bicg, label='BiCGStab', marker='o')\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Concept of preconditioning\n",
      "\n",
      "If $\\mathrm{cond}(A)$ is large, we need to use **preconditioners**.  \n",
      "\n",
      "Instead of solving $Ax = f$ we solve  \n",
      "\n",
      "$$\n",
      "AP^{-1} x = f,\n",
      "$$\n",
      "where $P y = z$ is easy to be solved (i.e. $P$ is diagonal, triangular, circulant ...).\n",
      "\n",
      "Another possibility to used **left preconditioner**:  \n",
      "$$ P A x = P f.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Jacobi method (as preconditioner)\n",
      "We can you as preconditioners some standard methods.  For example, **Jacobi method** uses  \n",
      "$$P = \\mathrm{diag}(A)$$ as preconditioner. Jacobi method has applications in multigrid."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gauss-Seidel (as preconditioner)\n",
      "Gauss-Seidel method. Given $A = A^{\\top} > 0$ we have  \n",
      "$$A = L + D + L^{\\top},$$\n",
      "where $D$ is the diagonal of $A$, $L$ is lower-triangular part, and Gauss-Seidel is $(L + D)^{-1}$.  \n",
      "Also, it can be shown that $\\rho(I + (L+D)^{-1} A) < 1.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Successive overrelaxation (as preconditioner)\n",
      "$$(D + w L) x = w b - (w U + (w-1) D) x,$$\n",
      "\n",
      "$$P = (D+wL)^{-1} (w U + (w-1) D).$$\n",
      "\n",
      "Optimal selection of $w$ is **not trivial**.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Incomplete LU (as preconditioner)\n",
      "\n",
      "Finally, the **incomplete LU** is typically the method of choice if you do not anything about your sparse matrix.  \n",
      "\n",
      "You start your Gaussian elimination and either: \n",
      "\n",
      "1. Throw away small elements (ILUT)\n",
      "2. Throw away elements that are not in the original sparse matrix ILU($0$)\n",
      "3. Throw away elements which are in the **neighbourhood** on the graph ILU(k)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ILU for Laplace2D: short demo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy\n",
      "n = 50\n",
      "ex = np.ones(n);\n",
      "lp1 = -sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
      "ee = scipy.sparse.eye(n, n)\n",
      "res_all = []\n",
      "res_all_bicg = []\n",
      "lp2 = scipy.sparse.kron(lp1, ee) + scipy.sparse.kron(ee, lp1)\n",
      "rhs = np.ones(n * n)\n",
      "\n",
      "def my_print(r):\n",
      "    res_all.append(r)\n",
      "\n",
      "def my_print2(x): #For BiCGStab they have another callback, please rewrite\n",
      "    res_all_bicg.append(np.linalg.norm(lp2.dot(x) - rhs))\n",
      "\n",
      "    \n",
      "M = scipy.sparse.linalg.spilu(lp2, drop_tol=1e-3, fill_factor=None)\n",
      "#Create prec\n",
      "lo = scipy.sparse.linalg.LinearOperator(lp2.shape, lambda x: M.solve(x))\n",
      "sol = scipy.sparse.linalg.gmres(lp2, rhs, callback=my_print, M=lo)\n",
      "plt.semilogy(res_all, marker='x',label='GMRES')\n",
      "sol2 = scipy.sparse.linalg.bicgstab(lp2, rhs, callback=my_print2, M=lo)\n",
      "plt.xlabel('Iteration number')\n",
      "plt.ylabel('Residual')\n",
      "plt.semilogy(res_all_bicg, label='BiCGStab', marker='o')\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some other possibilities\n",
      "- Algebraic multigrid\n",
      "- Sparse approximate inverse \n",
      "- Domain decomposition\n",
      "- For Toeplitz: Circulant!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Take home message\n",
      "- Krylov subspaces are methods of choice\n",
      "- CG + BiCGStab + GMRES are the methods to be used\n",
      "- Incomplete ILU is good for sparse matrices (software) and work typically \"not bad\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Next lecture\n",
      "- Matrix functions: what is that?\n",
      "- Matrix exponential: the \"queen\" of matrix functions \n",
      "- Methods to compute matrix functions (dense and sparse)\n",
      "- (Some) applications"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Questions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "html": [
        "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
        "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
        "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        /*width:80%;*/\n",
        "        /*margin-left:auto !important;\n",
        "        margin-right:auto;*/\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: 'Alegreya Sans', sans-serif;\n",
        "    }\n",
        "    h2 {\n",
        "        font-family: 'Fenix', serif;\n",
        "    }\n",
        "    h3{\n",
        "\t\tfont-family: 'Fenix', serif;\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "\th4{\n",
        "\t\tfont-family: 'Fenix', serif;\n",
        "       }\n",
        "    h5 {\n",
        "        font-family: 'Alegreya Sans', sans-serif;\n",
        "    }\t   \n",
        "    div.text_cell_render{\n",
        "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 1.2;\n",
        "        font-size: 160%;\n",
        "        /*width:70%;*/\n",
        "        /*margin-left:auto;*/\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\";\n",
        "\t\t\tfont-size: 90%;\n",
        "    }\n",
        "/*    .prompt{\n",
        "        display: None;\n",
        "    }*/\n",
        "    .text_cell_render h1 {\n",
        "        font-weight: 200;\n",
        "        font-size: 50pt;\n",
        "\t\tline-height: 110%;\n",
        "        color:#CD2305;\n",
        "        margin-bottom: 0.5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\t\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #CD2305;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "<IPython.core.display.HTML at 0x11252a990>"
       ]
      }
     ],
     "prompt_number": 123
    }
   ],
   "metadata": {}
  }
 ]
}